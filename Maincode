import cv2
import numpy as np
import os
from tensorflow.keras.models import load_model
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from mtcnn.mtcnn import MTCNN
import pickle

data_dir = r"C:\\Users\\asus\\Downloads\\Face antispoof\\processed\\"
users_photos_dir = "photos"
new_model_path = "improved_anti_spoofing_mobilenet.h5"
face_recognition_model_path = "face_recognition_model.pkl"
label_encoder_path = "label_encoder.pkl"

detector = MTCNN()


def extract_face_embedding(face_img, model):
    face_img = cv2.resize(face_img, (128, 128))
    face_img = face_img.astype("float32") / 255.0
    face_img = np.expand_dims(face_img, axis=0)
    embedding = model.layers[-3].output
    embedding_model = Model(inputs=model.input, outputs=embedding)
    return embedding_model.predict(face_img)[0]


# train anti-spoofing model
if os.path.exists(new_model_path):
    print(f"Loading existing anti-spoofing model from {new_model_path}")
    model = load_model(new_model_path)
else:
    print("Training new anti-spoofing model...")
    datagen = ImageDataGenerator(
        rescale=1. / 255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    train_gen = datagen.flow_from_directory(
        os.path.join(data_dir, 'train'),
        target_size=(128, 128),
        batch_size=32,
        class_mode='binary'
    )

    val_gen = datagen.flow_from_directory(
        os.path.join(data_dir, 'valid'),
        target_size=(128, 128),
        batch_size=32,
        class_mode='binary'
    )

    classes = np.array([0, 1])
    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_gen.classes)
    class_weights = {0: class_weights[0] * 1.5, 1: class_weights[1]}

    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)
    predictions = Dense(1, activation='sigmoid')(x)
    model = Model(inputs=base_model.input, outputs=predictions)

    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

    checkpoint = ModelCheckpoint(new_model_path, monitor='val_accuracy', save_best_only=True, mode='max')
    model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=20,
        callbacks=[checkpoint],
        class_weight=class_weights
    )
    print(f"Anti-spoofing model saved to {new_model_path}")


# Function to format person name for better display
def format_person_name(name):
    # Convert folder names to proper format (e.g., "john_doe" to "John Doe")
    parts = name.replace('_', ' ').split()
    return ' '.join(part.capitalize() for part in parts)


# --- Train or Load Face Recognition Model ---
if os.path.exists(face_recognition_model_path) and os.path.exists(label_encoder_path):
    print("Loading existing face recognition model...")
    with open(face_recognition_model_path, 'rb') as f:
        face_recognition_model = pickle.load(f)
    with open(label_encoder_path, 'rb') as f:
        le = pickle.load(f)
    # Get a mapping of person IDs to properly formatted names
    person_names = {i: format_person_name(name) for i, name in enumerate(le.classes_)}
else:
    print("Training new face recognition model...")
    embeddings = []
    labels = []
    person_count = {}  # Track number of photos per person

    # Ensure users_photos_dir exists
    if not os.path.exists(users_photos_dir):
        print(f"Error: Photos directory '{users_photos_dir}' not found!")
        exit(1)

    for person_name in os.listdir(users_photos_dir):
        person_dir = os.path.join(users_photos_dir, person_name)
        if not os.path.isdir(person_dir):
            continue

        # Initialize counter for this person
        person_count[person_name] = 0

        for img_file in os.listdir(person_dir):
            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(person_dir, img_file)
                img = cv2.imread(img_path)

                if img is None:
                    print(f"Warning: Couldn't read image {img_path}")
                    continue

                results = detector.detect_faces(img)
                if results:
                    result = max(results, key=lambda x: x['box'][2] * x['box'][3])
                    x, y, w, h = result['box']
                    margin = int(min(w, h) * 0.1)
                    face = img[max(0, y - margin):min(img.shape[0], y + h + margin),
                           max(0, x - margin):min(img.shape[1], x + w + margin)]

                    if face.size > 0:
                        embedding = extract_face_embedding(face, model)
                        embeddings.append(embedding)
                        labels.append(person_name)
                        person_count[person_name] += 1

        print(f"Processed {person_count[person_name]} images for {format_person_name(person_name)}")

    if not embeddings:
        print("No valid face embeddings found. Exiting.")
        exit(1)

    print(f"Total {len(embeddings)} face embeddings from {len(person_count)} people")

    embeddings = np.array(embeddings)
    labels = np.array(labels)

    le = LabelEncoder()
    encoded_labels = le.fit_transform(labels)

    face_recognition_model = SVC(kernel='linear', probability=True, C=10)
    face_recognition_model.fit(embeddings, encoded_labels)

    # Save the models for future use
    with open(face_recognition_model_path, 'wb') as f:
        pickle.dump(face_recognition_model, f)
    with open(label_encoder_path, 'wb') as f:
        pickle.dump(le, f)

    # Create mapping of person IDs to properly formatted names
    person_names = {i: format_person_name(name) for i, name in enumerate(le.classes_)}

    print("Face recognition model training complete and saved.")

# Print recognized people information
print("Recognized people:")
for person_id, formatted_name in person_names.items():
    print(f"ID: {person_id}, Name: {formatted_name}")

# Define confidence thresholds
SPOOF_THRESHOLD = 0.7  # Higher value = more strict (needs to be more "real")
RECOGNITION_THRESHOLD = 0.40  # Higher value = more strict (needs to be more confident)

# --- Real-Time Camera Recognition ---
cap = cv2.VideoCapture(0)

# For tracking stable recognition results
recognition_history = {}
HISTORY_SIZE = 5  # Number of frames to track for stability

print("Starting real-time recognition. Press 'q' to quit.")

while True:
    ret, frame = cap.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Create a copy for display (we'll add info panel)
    display_frame = frame.copy()

    # Add info panel to the right
    info_panel = np.zeros((frame.shape[0], 250, 3), dtype=np.uint8)
    cv2.putText(info_panel, "Recognized People:", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

    # Try to detect faces using MTCNN
    try:
        faces_mtcnn = detector.detect_faces(frame)
    except Exception as e:
        print(f"Face detection error: {e}")
        faces_mtcnn = []

    # Process each detected face
    person_seen = set()  # Track which people we've seen in this frame

    if faces_mtcnn:
        for i, face_info in enumerate(faces_mtcnn):
            x, y, w, h = face_info['box']
            face_key = f"face_{i}"

            # Skip faces that are too small
            if w < 50 or h < 50:
                continue

            # Extract the face
            face = frame[max(0, y):min(frame.shape[0], y + h),
                   max(0, x):min(frame.shape[1], x + w)]

            if face.size == 0:
                continue

            # Analyze for spoofing
            face_resized = cv2.resize(face, (128, 128))
            face_resized = face_resized.astype("float32") / 255.0
            face_resized = np.expand_dims(face_resized, axis=0)
            spoof_pred = model.predict(face_resized)[0][0]

            # Estimate authenticity score (higher = more real)
            authenticity = spoof_pred * 100

            # Prepare default label
            if spoof_pred > SPOOF_THRESHOLD:
                # Looks like a real face, try to recognize
                embedding = extract_face_embedding(face, model)
                user_probs = face_recognition_model.predict_proba([embedding])[0]
                user_id = np.argmax(user_probs)
                user_confidence = user_probs[user_id] * 100

                # Track the recognition results for this face position
                if face_key not in recognition_history:
                    recognition_history[face_key] = []

                if user_confidence > RECOGNITION_THRESHOLD * 100:
                    recognition_history[face_key].append((user_id, user_confidence))
                    # Keep only recent history
                    if len(recognition_history[face_key]) > HISTORY_SIZE:
                        recognition_history[face_key].pop(0)

                # If we have enough history, use the most frequent classification
                if len(recognition_history[face_key]) >= 3:
                    # Count occurrences of each user_id
                    id_counts = {}
                    for hist_id, _ in recognition_history[face_key]:
                        id_counts[hist_id] = id_counts.get(hist_id, 0) + 1

                    # Get the most common user_id
                    stable_user_id = max(id_counts.items(), key=lambda x: x[1])[0]

                    # Calculate average confidence for this user_id
                    avg_confidence = np.mean(
                        [conf for uid, conf in recognition_history[face_key] if uid == stable_user_id])

                    # Use the stable ID if it appears in majority of frames
                    if id_counts[stable_user_id] > len(recognition_history[face_key]) / 2:
                        user_id = stable_user_id
                        user_confidence = avg_confidence

                formatted_name = person_names.get(user_id, "Unknown")

                if user_confidence > RECOGNITION_THRESHOLD * 100:
                    label = f"{formatted_name}"
                    sub_label = f"Conf: {user_confidence:.1f}%"
                    color = (0, 255, 0)  # Green
                    person_seen.add(user_id)
                else:
                    label = "Unknown Person"
                    sub_label = f"Auth: {authenticity:.1f}%"
                    color = (0, 165, 255)  # Orange
            else:
                # Failed spoofing check
                label = "Spoof Detected"
                sub_label = f"Auth: {authenticity:.1f}%"
                color = (0, 0, 255)  # Red

                # Clear recognition history for this face
                if face_key in recognition_history:
                    recognition_history[face_key] = []

            # Draw rectangles and labels on the face
            cv2.rectangle(display_frame, (x, y), (x + w, y + h), color, 2)

            # Create background for text
            text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(display_frame,
                          (x, y - 30),
                          (x + text_size[0] + 10, y),
                          color, -1)

            # Draw main label
            cv2.putText(display_frame, label, (x + 5, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            # Draw sub-label below the face
            cv2.putText(display_frame, sub_label, (x, y + h + 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    # Update info panel with recognized people
    panel_y = 60
    for user_id, formatted_name in person_names.items():
        status = "PRESENT" if user_id in person_seen else "ABSENT"
        status_color = (0, 255, 0) if user_id in person_seen else (0, 0, 255)

        cv2.putText(info_panel, formatted_name, (10, panel_y),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(info_panel, status, (150, panel_y),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color, 1)
        panel_y += 30

    # Show auth threshold
    cv2.putText(info_panel, f"Auth Threshold: {SPOOF_THRESHOLD * 100:.0f}%", (10, panel_y + 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    # Show confidence threshold
    cv2.putText(info_panel, f"Conf Threshold: {RECOGNITION_THRESHOLD * 100:.0f}%", (10, panel_y + 60),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    # Add instructions
    cv2.putText(info_panel, "Press 'q' to quit", (10, info_panel.shape[0] - 20),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    # Combine frame and info panel
    combined_frame = np.hstack((display_frame, info_panel))

    # Scale down if too large for display
    screen_width = 1280  # Assume standard screen width
    if combined_frame.shape[1] > screen_width:
        scale = screen_width / combined_frame.shape[1]
        combined_frame = cv2.resize(combined_frame, (0, 0), fx=scale, fy=scale)

    cv2.imshow("Face Recognition & Anti-Spoofing", combined_frame)

    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break

print("Exiting program")
cap.release()
cv2.destroyAllWindows()
